{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "object_detection_mobilenet_capybara_tutorial.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sourangshupal/Tensorflow2-Object-Detection-Tutorial/blob/master/object_detection_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8-yl-s-WKMG"
      },
      "source": [
        "# Tutorial para Detecção de Objetos utilizando TensorFlow 2.x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cIrseUv6WKz"
      },
      "source": [
        "Este tutorial é baseado nos trabalhos de [Hassan Rafiq](https://medium.com/swlh/image-object-detection-tensorflow-2-object-detection-api-af7244d4c34e) e [Dat Tran](https://towardsdatascience.com/how-to-train-your-own-object-detector-with-tensorflows-object-detector-api-bec72ecfe1d9)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3UGXxUii5Ym"
      },
      "source": [
        "## Checando & Instalando os pacotes\n",
        "\n",
        "Primeiro passo é atualizar o gerenciador de pacotes `pip` para, em seguida, instalarmos os pacotes necessários."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ukro5Dsc7Bd"
      },
      "source": [
        "!lsb_release -a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python-headless==4.1.2.30"
      ],
      "metadata": {
        "id": "5XqsrmTvfJYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgFjqNDWb_tW"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f8PS3qimves"
      },
      "source": [
        "!pip install -U pip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRMk1VZCZ-Ep"
      },
      "source": [
        "### Instalando tf slim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9cfkPerneWE"
      },
      "source": [
        "Vamos instalar o tf-slim, uyma API de alto nível para o tensorflow, utilizada para definição, treinamento e avaliação de modelos complexos. Mais informações em [https://github.com/google-research/tf-slim.](https://github.com/google-research/tf-slim)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGL97-GXjSUw"
      },
      "source": [
        "#!pip install -U --pre tensorflow==\"2.*\"\n",
        "!pip install tf_slim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ez11ANotoWe-"
      },
      "source": [
        "### Verificando a Versão do Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zgkmZjvIUlM"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdognrL8ocw1"
      },
      "source": [
        "A saída esperada é Tensorflow==2.6.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X293P_lohwH"
      },
      "source": [
        "### Verificando se existe uma GPU disponível"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mzZU4qIonDmA"
      },
      "source": [
        "tf.test.is_gpu_available() # True/False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPrB1taqsuWZ"
      },
      "source": [
        "tf.test.gpu_device_name()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6ajFAuBswcS"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_ap_s9ajTHH"
      },
      "source": [
        "Vamos instalar o pacote `pycocotools`, uma API para ler e manipular as  notações do conjunto de imagens [COCO](https://cocodataset.org/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bg8ZyA47i3pY"
      },
      "source": [
        "!pip install pycocotools"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZcXhz4Opwi0"
      },
      "source": [
        "Mais informações em [https://medium.com/porto-seguro/precisa-de-imagens-para-treinar-sua-ia-use-a-api-do-coco-dataset-pycocotools-a87c7e21774b](https://medium.com/porto-seguro/precisa-de-imagens-para-treinar-sua-ia-use-a-api-do-coco-dataset-pycocotools-a87c7e21774b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_W6-zFuj0zf"
      },
      "source": [
        "## Montando o Google Drive para armazenamento dos dados\n",
        "\n",
        "Para montar sua pasta do Google Drive, você precisa logar na sua conta e fornecer um código de verificação, que será solicitado após a execução da célula a seguir."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9N7emOVFdPjt"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trMEezzHqVae"
      },
      "source": [
        "Vamos criar um diretório no Google Drive, chamada `TFOD2`, a qual será nosso diretório de trabalho"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoIgocgAe0Ai"
      },
      "source": [
        "import os\n",
        "os.makedirs('/content/drive/MyDrive/TFOD2', exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QB5l3JYiqtA0"
      },
      "source": [
        "## Instalação API Object Detection \n",
        "\n",
        "Vamos baixar o repositório oficial do **Tensorflow**, disponível no github, mais especificiamente a pasta `tensorflow/models` o qual contém a API para detecção de objetos.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykA0c-om51s1"
      },
      "source": [
        "import os\n",
        "import pathlib\n",
        "\n",
        "os.chdir('/content/')\n",
        "\n",
        "if \"models\" in pathlib.Path.cwd().parts:\n",
        "  while \"models\" in pathlib.Path.cwd().parts:\n",
        "    os.chdir('..')\n",
        "    \n",
        "elif not pathlib.Path('models').exists():\n",
        "  !git clone --depth 1 https://github.com/tensorflow/models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O219m6yWAj9l"
      },
      "source": [
        "Agora, vamos compilar o protobufs e instalar o pacote para Object Detection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PY41vdYYNlXc"
      },
      "source": [
        "###%%bash\n",
        "os.chdir('/content/models/research/')\n",
        "!protoc object_detection/protos/*.proto --python_out=."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJ7pPh8Ssnuk"
      },
      "source": [
        "Para saber mais sobre o protobufs, visite [https://medium.com/trainingcenter/protobuf-uma-alternativa-ao-json-e-xml-a35c66edab4d](https://medium.com/trainingcenter/protobuf-uma-alternativa-ao-json-e-xml-a35c66edab4d).\n",
        "\n",
        "Agora, vamos iniciar a instalação doa API do Tensorflow para Object Detection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6xYJuP1kA9-"
      },
      "source": [
        "!cp /content/models/research/object_detection/packages/tf2/setup.py /content/models/research\n",
        "os.chdir('/content/models/research/')\n",
        "!python -m pip install ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYKrUv0CpO2x"
      },
      "source": [
        "## Testando a instalação da API Object Detection\n",
        "\n",
        "Executando a célula a seguir, será feito um teste que verifica se nossa instalação está correta."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9w8rQhFkoxO"
      },
      "source": [
        "os.chdir('/content/models/research/')\n",
        "!python object_detection/builders/model_builder_tf2_test.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBdjK2G5ywuc"
      },
      "source": [
        "## Inferência: Detecção de Objetos utilizando modelo pré-treinado\n",
        "\n",
        "Vamos realizar o processo de inferência para realizar a detecção de objetos utilizando o modelo pré-treinado no dataset [COCO](https://cocodataset.org/).\n",
        "\n",
        "COCO é um dataset padrão para treinamento dos modelos para object detection, considerado um benchmark para comparação dos modelos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgFZvERwut4o"
      },
      "source": [
        "Importando os pacotes necessários para inferência:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hV4P5gyTWKMI"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import six.moves.urllib as urllib\n",
        "import sys\n",
        "import tarfile\n",
        "import tensorflow as tf\n",
        "import zipfile\n",
        "\n",
        "from collections import defaultdict\n",
        "from io import StringIO\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "from IPython.display import display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5FNuiRPWKMN"
      },
      "source": [
        "Importando o módulo de object detection:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGORnL1svFhF"
      },
      "source": [
        "os.chdir('/content/models/research/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-IMl4b6BdGO"
      },
      "source": [
        "from object_detection.utils import ops as utils_ops\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as vis_util"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYPCiag2iz_q"
      },
      "source": [
        "Importando os Patches necessários."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mF-YlMl8c_bM"
      },
      "source": [
        "# patch tf1 into `utils.ops`\n",
        "utils_ops.tf = tf.compat.v1\n",
        "\n",
        "# Patch the location of gfile\n",
        "tf.gfile = tf.io.gfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfn_tRFOWKMO"
      },
      "source": [
        "## Preparando o Modelo\n",
        "\n",
        "Vamos definir algumas funções necessárias para baixar o modelo e realizar o processo de inferência. Para isso, devemos escolher um modelo disponível em [detection model zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md). Nessa lista, podemos baixar modelos que forma exportados utilizando a ferramenta `export_inference_graph.py`, que permite realizar facilmente seu carregamento. Cada modelo possui uma determinada velocidade de execução e acurária, o que restringe sua aplicação.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ai8pLZZWKMS"
      },
      "source": [
        "### Definindo um Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zm8xp-0eoItE"
      },
      "source": [
        "# Verify: https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md\n",
        "def load_model(model_name):\n",
        "  base_url = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/'\n",
        "  model_file = model_name + '.tar.gz'\n",
        "  model_dir = tf.keras.utils.get_file(\n",
        "    fname=model_name, \n",
        "    origin=base_url + model_file,\n",
        "    untar=True)\n",
        "\n",
        "  model_dir = pathlib.Path(model_dir)/\"saved_model\"\n",
        "\n",
        "  model = tf.saved_model.load(str(model_dir))\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1MVVTcLWKMW"
      },
      "source": [
        "## Carregando um Label Map\n",
        "\n",
        "Label Maps realizam o mapeamento de uma lista de índices para uma lista de categorias, portanto, quando nosso modelo predizer uma classe `5`, nós saberemos que se trata da classe `airplane`. Se preferir substituir por seu próprio Label Maps, basta criar uma função que retorne um dicionário, que mapeie números inteiros para o label das classes apropriadas.\n",
        "\n",
        "Como o modelo que utilizaremos foi treinado no dataset COCO, vamos utilizar o Label Maps deste dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDbpHkiWWKMX"
      },
      "source": [
        "# List of the strings that is used to add correct label for each box.\n",
        "PATH_TO_LABELS = '/content/models/research/object_detection/data/mscoco_label_map.pbtxt'\n",
        "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVU3U_J6IJVb"
      },
      "source": [
        "For the sake of simplicity we will test on 2 images:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0_1AGhrWKMc"
      },
      "source": [
        "## Carregando o modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7aOtOlebK7h"
      },
      "source": [
        "Vamos definir um modelo, que será baixado e carregado na memória. Verifique um modelo dentre os disponíveis no  [Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md) do respositório oficial. \n",
        "\n",
        "Por exemplo, para baixar o modelo `mobilenet`, disponível no link \n",
        "\n",
        "[http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz](http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz)\n",
        "\n",
        "basta indicar o nome do arquivo:\n",
        "```\n",
        "model_name = 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8'\n",
        "```\n",
        "\n",
        "Desta forma, você pode escolher e definir um modelo a ser utilizado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XNT0wxybKR6"
      },
      "source": [
        "# os.chdir('/content/drive/MyDrive/TFOD2/')\n",
        "# http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d0_coco17_tpu-32.tar.gz\n",
        "model_name = 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8'\n",
        "detection_model = load_model(model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yN1AYfAEJIGp"
      },
      "source": [
        "Verificando a assinatura da entrada do modelo. Espera-se que receba um batch de imagens de 3 cores do tipo uint8:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CK4cnry6wsHY"
      },
      "source": [
        "print(detection_model.signatures['serving_default'].inputs)\n",
        "print(detection_model.signatures['serving_default'].output_dtypes)\n",
        "print(detection_model.signatures['serving_default'].output_shapes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JP5qZ7sXJpwG"
      },
      "source": [
        "Adicionando uma função wrapper para executar o processo de inferência utilizando o modelo escolhido sobre uma única imagem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajmR_exWyN76"
      },
      "source": [
        "def run_inference_for_single_image(model, image):\n",
        "  image = np.asarray(image)\n",
        "  # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
        "  input_tensor = tf.convert_to_tensor(image)\n",
        "  # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
        "  input_tensor = input_tensor[tf.newaxis,...]\n",
        "\n",
        "  # Run inference\n",
        "  model_fn = model.signatures['serving_default']\n",
        "  output_dict = model_fn(input_tensor)\n",
        "\n",
        "  # All outputs are batches tensors.\n",
        "  # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
        "  # We're only interested in the first num_detections.\n",
        "  num_detections = int(output_dict.pop('num_detections'))\n",
        "  output_dict = {key:value[0, :num_detections].numpy() \n",
        "                 for key,value in output_dict.items()}\n",
        "  output_dict['num_detections'] = num_detections\n",
        "\n",
        "  # detection_classes should be ints.\n",
        "  output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
        "   \n",
        "  # Handle models with masks:\n",
        "  if 'detection_masks' in output_dict:\n",
        "    # Reframe the the bbox mask to the image size.\n",
        "    detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
        "              output_dict['detection_masks'], output_dict['detection_boxes'],\n",
        "               image.shape[0], image.shape[1])      \n",
        "    detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5,\n",
        "                                       tf.uint8)\n",
        "    output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
        "    \n",
        "  return output_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1wq0LVyMRR_"
      },
      "source": [
        "Definindo uma função para exibição do resultado do processo de inferência."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWh_1zz6aqxs"
      },
      "source": [
        "def show_inference(model, image_path):\n",
        "  # the array based representation of the image will be used later in order to prepare the\n",
        "  # result image with boxes and labels on it.\n",
        "  image_np = np.array(Image.open(image_path))\n",
        "  # Actual detection.\n",
        "  output_dict = run_inference_for_single_image(model, image_np)\n",
        "  # Visualization of the results of a detection.\n",
        "  vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "      image_np,\n",
        "      output_dict['detection_boxes'],\n",
        "      output_dict['detection_classes'],\n",
        "      output_dict['detection_scores'],\n",
        "      category_index,\n",
        "      instance_masks=output_dict.get('detection_masks_reframed', None),\n",
        "      use_normalized_coordinates=True,\n",
        "      line_thickness=8)\n",
        "\n",
        "  display(Image.fromarray(image_np))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRmNybtY7gr-"
      },
      "source": [
        "### Teste de Inferência\n",
        "\n",
        "Vamos definir algumas imagens disponíveis em uma pasta de teste da própria API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jG-zn5ykWKMd"
      },
      "source": [
        "# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.\n",
        "PATH_TO_TEST_IMAGES_DIR = pathlib.Path('/content/models/research/object_detection/test_images')\n",
        "TEST_IMAGE_PATHS = sorted(list(PATH_TO_TEST_IMAGES_DIR.glob(\"*.jpg\")))\n",
        "TEST_IMAGE_PATHS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTyNwmj67tsZ"
      },
      "source": [
        "Em seguida, vamos executar o processo de inferência sobre essas imagens:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a5wMHN8WKMh"
      },
      "source": [
        "for image_path in TEST_IMAGE_PATHS:\n",
        "  show_inference(detection_model, image_path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MH_VTbbX7x6f"
      },
      "source": [
        "Para enviar uma imagem e executar o processo de inferência:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSliMv2XQ8FX"
      },
      "source": [
        "from google.colab import files\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  show_inference(detection_model, fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uD57gEIkehEX"
      },
      "source": [
        "## Criação e Preparação de um dataset customizado\n",
        "\n",
        "Para criar o seu próprio dataset para detecção de objetos é necessário selecionar um conjunto de imagens do objeto que se deseja detectar e realizar a anotação das posições dos objetivos da imagem. Para isso, existe uma ferramenta chamada [LabelImg](https://github.com/tzutalin/labelImg) que facilita a etapa de anotação. \n",
        "\n",
        "Neste tutorial, utilizamos o formato XML para as anotações, conforme o padrão definido pelo dataset **Pascal VOC**. \n",
        "\n",
        "Para mais informações sobre o formado Pascal VOC, verifique [https://towardsdatascience.com/coco-data-format-for-object-detection-a4c5eaf518c5](https://towardsdatascience.com/coco-data-format-for-object-detection-a4c5eaf518c5).\n",
        "\n",
        "Um tutorial para criação dos scripts  de exportação das anotações pode ser verificado em [https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/using_your_own_dataset.md](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/using_your_own_dataset.md)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcCrR9QT_r-o"
      },
      "source": [
        "Para este tutorial, utilizaremos o dataset [Raccoon Dataset](https://github.com/datitran/raccoon_dataset), que contém 200 imagens de guaxinins. No respositório deste dataset existe um conjunto de ferramentas que realizam a exportação das anotaçãos para o formato TFRecord, utilizado pelo Tensorflow.\n",
        "\n",
        "Para mais informações sobre o formato TFRecord, verifique [https://blog.roboflow.com/create-tfrecord/](https://blog.roboflow.com/create-tfrecord/).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrBsRPKejkGa"
      },
      "source": [
        "# Source: https://medium.com/swlh/image-object-detection-tensorflow-2-object-detection-api-af7244d4c34e\n",
        "import os\n",
        "os.chdir('/content/')\n",
        "!git clone https://github.com/freds0/capybara_dataset.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PffdpjVJslDs"
      },
      "source": [
        "!ls capybara_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aol2ec4ldPes"
      },
      "source": [
        "### Covertendo XML para CSV\n",
        "\n",
        "Dentre as ferramentas disponíveis no dataset [Raccoon Dataset](https://github.com/datitran/raccoon_dataset), utilizaremos as funções disponíveis em `generate_tfrecord.py`, especificamente as funções que convertem um arquivo XML para o formato CSV, que será posteriormente exportado para o formato TFRecord. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJFrXdWcj8Wm"
      },
      "source": [
        "\"\"\"\n",
        "source: https://github.com/datitran/raccoon_dataset/blob/master/generate_tfrecord.py\n",
        "\n",
        "Usage:\n",
        "  # From tensorflow/models/\n",
        "  # Create train data:\n",
        "  python generate_tfrecord.py --csv_input=data/train_labels.csv  --output_path=train.record\n",
        "  # Create test data:\n",
        "  python generate_tfrecord.py --csv_input=data/test_labels.csv  --output_path=test.record\n",
        "\"\"\"\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "\n",
        "import os\n",
        "import io\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from PIL import Image\n",
        "from object_detection.utils import dataset_util\n",
        "from collections import namedtuple, OrderedDict\n",
        "\n",
        "# TO-DO replace this with label map\n",
        "def class_text_to_int(row_label):\n",
        "    if row_label == 'capivara':\n",
        "        return 1\n",
        "    else:\n",
        "        None\n",
        "\n",
        "def split(df, group):\n",
        "    data = namedtuple('data', ['filename', 'object'])\n",
        "    gb = df.groupby(group)\n",
        "    return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n",
        "\n",
        "\n",
        "def create_tf_example(group, path):\n",
        "    with tf.io.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n",
        "        encoded_jpg = fid.read()\n",
        "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
        "    image = Image.open(encoded_jpg_io)\n",
        "    width, height = image.size\n",
        "\n",
        "    filename = group.filename.encode('utf8')\n",
        "    image_format = b'jpg'\n",
        "    xmins = []\n",
        "    xmaxs = []\n",
        "    ymins = []\n",
        "    ymaxs = []\n",
        "    classes_text = []\n",
        "    classes = []\n",
        "\n",
        "    for index, row in group.object.iterrows():\n",
        "        xmins.append(row['xmin'] / width)\n",
        "        xmaxs.append(row['xmax'] / width)\n",
        "        ymins.append(row['ymin'] / height)\n",
        "        ymaxs.append(row['ymax'] / height)\n",
        "        classes_text.append(row['class'].encode('utf8'))\n",
        "        classes.append(class_text_to_int(row['class']))\n",
        "\n",
        "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
        "        'image/height': dataset_util.int64_feature(height),\n",
        "        'image/width': dataset_util.int64_feature(width),\n",
        "        'image/filename': dataset_util.bytes_feature(filename),\n",
        "        'image/source_id': dataset_util.bytes_feature(filename),\n",
        "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
        "        'image/format': dataset_util.bytes_feature(image_format),\n",
        "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
        "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
        "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
        "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
        "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
        "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
        "    }))\n",
        "    return tf_example\n",
        "\n",
        "\n",
        "def generate_tfrecord(output_path, image_dir, csv_input):\n",
        "    writer = tf.io.TFRecordWriter(output_path)\n",
        "    path = os.path.join(image_dir)\n",
        "    examples = pd.read_csv(csv_input)\n",
        "    grouped = split(examples, 'filename')\n",
        "    for group in grouped:\n",
        "        tf_example = create_tf_example(group, path)\n",
        "        writer.write(tf_example.SerializeToString())\n",
        "\n",
        "    writer.close()\n",
        "    output_path = os.path.join(os.getcwd(), output_path)\n",
        "    print('Successfully created the TFRecords: {}'.format(output_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acSVs5O6dX1s"
      },
      "source": [
        "### Exportando os arquivos de treino para o formato TFRecord\n",
        "\n",
        "Nesta etapa, realizaremos a conversão dos arquivos CSV para o formato TFrecord, utilizando a `generate_tfrecord`definida na célula anterior. Os arquivos CSV encontram-se disponíveis no Raccoon Dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IIXDiM1GS3d"
      },
      "source": [
        "!ls /content/capybara_dataset/data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsK7QLTkCeyo"
      },
      "source": [
        "Vamos exportar os arquivos de treino:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2p5vK2DzluTB"
      },
      "source": [
        "# !python generate_tfrecord.py --csv_input=images/train_labels.csv --image_dir=images/train --output_path=train.record\n",
        "\n",
        "##Run on training data\n",
        "generate_tfrecord('/content/capybara_dataset/data/train.record', '/content/capybara_dataset/images/train/' , '/content/capybara_dataset/data/train_labels.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GbwgwFqodslO"
      },
      "source": [
        "Em seguida, vamos exportar os arquivos de teste:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5U5srvxmmDf"
      },
      "source": [
        "#!python generate_tfrecord.py --csv_input=images/test_labels.csv --image_dir=images/test --output_path=test.record\n",
        "generate_tfrecord('/content/capybara_dataset/data/test.record', '/content/capybara_dataset/images/test' , '/content/capybara_dataset/data/test_labels.csv')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLHLVyG_dwCj"
      },
      "source": [
        "## Download e Treinamento do Modelo\n",
        "\n",
        "Precisamos definir um modelo no qual será realizado *fine tuning* com o novo dataset. Escolha um modelo dentre os disponíveis no endereço [https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md). \n",
        "\n",
        "Neste tutorial, vamos escolher o modelo **mobilenet_v2**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyqtqVvCm64F"
      },
      "source": [
        "os.chdir('/content/')\n",
        "#!wget http://download.tensorflow.org/models/object_detection/classification/tf2/20200710/mobilenet_v2.tar.gz\n",
        "!wget http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3A8NIL8d3ix"
      },
      "source": [
        "Descompactando o modelo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPmdYs-qnLcD"
      },
      "source": [
        "import tarfile\n",
        "#!tar -xvf mobilenet_v2.tar.gz\n",
        "tar = tarfile.open(\"ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz\")\n",
        "tar.extractall()\n",
        "tar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4RxUP5DQZe1"
      },
      "source": [
        "os.listdir('ssd_mobilenet_v2_320x320_coco17_tpu-8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndUv66BsFZqo"
      },
      "source": [
        "## Arquivos de Configuração\n",
        "\n",
        "Para realizar o treinamento do modelo escolhido, devemos configurar dois arquivos:\n",
        "- Label Maps\n",
        "- Training Pipeline\n",
        "\n",
        "### Label Maps\n",
        "\n",
        "O arquivo Label Maps contém o dicionário das classes presentes no dataset. No Capybara Dataset esse arquivo possui o seguinte formato:\n",
        "\n",
        "```\n",
        "item {\n",
        "  id: 1\n",
        "  name: 'capivara'\n",
        "}\n",
        "```\n",
        "Exemplos de arquivos Label Maps podem ser encontrados [neste link](https://github.com/tensorflow/models/tree/master/research/object_detection/data).\n",
        "É importante notar que o label map deve sempre começar com id=1, conforme discutido [neste link] (https://github.com/tensorflow/models/issues/1696).\n",
        "\n",
        "### Training Pipeline\n",
        "\n",
        "O arquivo de Training Pipeline contém os parâmetros e hiperparâmetros para treinamento do modelo. \n",
        "\n",
        "Para mais informações sobre o pipeline de treinamento, verifique o conteúdo oficial do Tensorflow, disponível [neste link](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/configuring_jobs.md).\n",
        "\n",
        "Para isso, utilizamos um arquivo de exemplo, que é específicio para o modelo escolhido. Os arquivos para cada modelo estão disponíveis [neste link](https://github.com/tensorflow/models/tree/master/research/object_detection/samples/configs).\n",
        "\n",
        "Para o modelo Mobilenet, utilizaremos o arquivo disponível no [repositório oficial do Tensorflow](https://github.com/tensorflow/models/blob/master/research/object_detection/samples/configs/ssd_mobilenet_v2_coco.config).\n",
        "\n",
        "É necessário alterar o número de classes:\n",
        "\n",
        "```\n",
        "# Antes\n",
        "num_classes: 90\n",
        "# Depois\n",
        "num_classes: 1\t\n",
        "```\n",
        "\n",
        "```\n",
        "# Antes\n",
        "fine_tune_checkpoint: \"PATH_TO_BE_CONFIGURED\"\n",
        "# Depois\n",
        "fine_tune_checkpoint: \"/content/mobilenet_v2/mobilenet_v2.ckpt-1\"\t\t##-- Edit( check downloaded model)\n",
        "```\n",
        "\n",
        "Em seguida, os paths dos arquivos de treinamento:\n",
        "```\n",
        "# Antes\n",
        "train_input_reader: {\n",
        "  label_map_path: \"PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt\"\n",
        "  tf_record_input_reader {\n",
        "    input_path: \"PATH_TO_BE_CONFIGURED/mscoco_train.record-?????-of-00100\"\n",
        "  }  \n",
        "}\n",
        "\n",
        "# Depois\n",
        "train_input_reader: {\n",
        "  label_map_path: \"/content/capybara_dataset/data/object-detection.pbtxt\"\n",
        "  tf_record_input_reader {\n",
        "    input_path: \"/content/capybara_dataset/data/train.record\"\n",
        "  }\n",
        "}\n",
        "```\n",
        "E os paths dos arquivos de teste:\n",
        "```\n",
        "# Antes\n",
        "eval_input_reader: {\n",
        "  label_map_path: \"PATH_TO_BE_CONFIGURED/mscoco_label_map.pbtxt\"\n",
        "  shuffle: false\n",
        "  num_epochs: 1\n",
        "  tf_record_input_reader {\n",
        "    input_path: \"PATH_TO_BE_CONFIGURED/mscoco_val.record-?????-of-00010\"\n",
        "  }\n",
        "}\n",
        "\n",
        "# Depois\n",
        "eval_input_reader: {\n",
        "  label_map_path: \"/content/capybara_dataset/data/object-detection.pbtxt\"\n",
        "  shuffle: false\n",
        "  num_epochs: 1\n",
        "  tf_record_input_reader {\n",
        "    input_path: \"/content/capybara_dataset/data/test.record\" \n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "Também é necessário definir um `batch_size` adequado à GPU disponível. Para uma Tesla T4, com aproximadamente 12G de memória dedicada, utilizamos o seguinte:\n",
        "```\n",
        "batch_size: 80\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipytn2ard8P-"
      },
      "source": [
        "### Iniciando o Treinamento do Modelo\n",
        "\n",
        "Por fim, precisamos apenas definir as ultimas configurações."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrM5sgmxGr-v"
      },
      "source": [
        "os.chdir('/content/models/research/object_detection/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFmuATYKodUS"
      },
      "source": [
        "!python model_main_tf2.py \\\n",
        "    --model_dir='/content/training_mobilenet_v2_capybara_dataset/' \\\n",
        "    --pipeline_config_path='/content/ssd_mobilenet_v2_320x320_coco17_tpu-8/pipeline.config' \\\n",
        "    --checkpoint_every_n=10000 \\\n",
        "    --num_train_steps=10000000 \\\n",
        "    --num_workers=32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsbMqhqReB02"
      },
      "source": [
        "### Avaliando o Modelo Customizado\n",
        "\n",
        "Após o treinamento, é possível avaliar os resultados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DL8y8tKWroG_"
      },
      "source": [
        "!python model_main_tf2.py \\\n",
        "    --model_dir='/content/drive/MyDrive/TFOD2/training_demo/mobilenet_v2/' \\\n",
        "    --pipeline_config_path='/content/mobilenet_v2_pipeline.config' \\\n",
        "    --checkpoint_dir='/content/mobilenet_v2_evaluation/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSPSpKDLRjRF"
      },
      "source": [
        "Em seguida, utilize o Tensorboard para avaliar os resultados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxwsV3sdecLO"
      },
      "source": [
        "### Exportandos os checkpoints\n",
        "\n",
        "Também é possíve exportar os checkpoints do modelo treinado, assim é possível enviar para uma outra plataforma a fim de realizar seu deploy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJr2Mon4xOTj"
      },
      "source": [
        "os.listdir('/content/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3qmRtUEbH-n"
      },
      "source": [
        "### Exporting the Infernece Graph\n",
        "!python exporter_main_v2.py \\\n",
        "  --input_type image_tensor \\\n",
        "  --pipeline_config_path='/content/mobilenet_v2_pipeline.config' \\\n",
        "  --trained_checkpoint_dir='/content/mobilenet_v2_capybara_dataset/' \\\n",
        "  --output_directory='/content/mobilenet_v2_capybara_dataset/exported/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdOX69aAibZF"
      },
      "source": [
        "## Inferência: Detecção de Objetos utilizando modelo treinado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TlTE3xOihPv"
      },
      "source": [
        "import pathlib \n",
        "\n",
        "def load_custom_model(model_name):\n",
        "  model_file = model_name\n",
        "  model_dir = pathlib.Path(model_file)/\"saved_model\"\n",
        "  model = tf.saved_model.load(str(model_dir))\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e76-5kcUZ-b"
      },
      "source": [
        "from object_detection.utils import ops as utils_ops\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as vis_util"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UGOkqlljNDA"
      },
      "source": [
        "# List of the strings that is used to add correct label for each box.\n",
        "PATH_TO_LABELS = '/content/capybara_dataset/data/object-detection.pbtxt'\n",
        "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0wnSdpBjMda"
      },
      "source": [
        "model_name = '/content/mobilenet_v2_capybara_dataset/exported/'\n",
        "detection_model = load_custom_model(model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yz6V_2OzlSGQ"
      },
      "source": [
        "print(detection_model.signatures['serving_default'].inputs)\n",
        "print(detection_model.signatures['serving_default'].output_dtypes)\n",
        "print(detection_model.signatures['serving_default'].output_shapes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBYAwQeLlR6Q"
      },
      "source": [
        "def run_inference_for_single_image(model, image):\n",
        "  image = np.asarray(image)\n",
        "  # The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
        "  input_tensor = tf.convert_to_tensor(image)\n",
        "  # The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
        "  input_tensor = input_tensor[tf.newaxis,...]\n",
        "\n",
        "  # Run inference\n",
        "  model_fn = model.signatures['serving_default']\n",
        "  output_dict = model_fn(input_tensor)\n",
        "\n",
        "  # All outputs are batches tensors.\n",
        "  # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
        "  # We're only interested in the first num_detections.\n",
        "  num_detections = int(output_dict.pop('num_detections'))\n",
        "  output_dict = {key:value[0, :num_detections].numpy() \n",
        "                 for key,value in output_dict.items()}\n",
        "  output_dict['num_detections'] = num_detections\n",
        "\n",
        "  # detection_classes should be ints.\n",
        "  output_dict['detection_classes'] = output_dict['detection_classes'].astype(np.int64)\n",
        "   \n",
        "  # Handle models with masks:\n",
        "  if 'detection_masks' in output_dict:\n",
        "    # Reframe the the bbox mask to the image size.\n",
        "    detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
        "              output_dict['detection_masks'], output_dict['detection_boxes'],\n",
        "               image.shape[0], image.shape[1])      \n",
        "    detection_masks_reframed = tf.cast(detection_masks_reframed > 0.5,\n",
        "                                       tf.uint8)\n",
        "    output_dict['detection_masks_reframed'] = detection_masks_reframed.numpy()\n",
        "    \n",
        "  return output_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDWT0cGElRuu"
      },
      "source": [
        "def show_inference(model, image_path):\n",
        "  # the array based representation of the image will be used later in order to prepare the\n",
        "  # result image with boxes and labels on it.\n",
        "  image_np = np.array(Image.open(image_path))\n",
        "  # Actual detection.\n",
        "  output_dict = run_inference_for_single_image(model, image_np)\n",
        "  # Visualization of the results of a detection.\n",
        "  vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "      image_np,\n",
        "      output_dict['detection_boxes'],\n",
        "      output_dict['detection_classes'],\n",
        "      output_dict['detection_scores'],\n",
        "      category_index,\n",
        "      instance_masks=output_dict.get('detection_masks_reframed', None),\n",
        "      use_normalized_coordinates=True,\n",
        "      line_thickness=8)\n",
        "\n",
        "  display(Image.fromarray(image_np))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlxwrQd2S4ME"
      },
      "source": [
        "### Enviando um Arquivo para Inferência"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7attiSMMaM77"
      },
      "source": [
        "from google.colab import files\n",
        "from io import BytesIO\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  show_inference(detection_model, fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rs1SqqktS9ww"
      },
      "source": [
        "### Inferência nos arquivos de exemplo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLRsbU1ijMux"
      },
      "source": [
        "# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.\n",
        "PATH_TO_TEST_IMAGES_DIR = pathlib.Path('/content/capybara_dataset/images/test')\n",
        "TEST_IMAGE_PATHS = sorted(list(PATH_TO_TEST_IMAGES_DIR.glob(\"*.jpeg\")))\n",
        "TEST_IMAGE_PATHS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Kj_Mv88lp6T"
      },
      "source": [
        "for image_path in TEST_IMAGE_PATHS:\n",
        "  show_inference(detection_model, image_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NP5NAkXu1w9Q"
      },
      "source": [
        "import shutil\n",
        "import os\n",
        "os.chdir('/content/mobilenet_v2_capybara_dataset')\n",
        "output_zip_filename = 'mobilenet_v2_capybara_dataset'\n",
        "shutil.make_archive(output_zip_filename, 'zip', 'exported')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVrus5r3Y2Ur"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "zip_file = '/content/mobilenet_v2_capybara_dataset/mobilenet_v2_capybara_dataset.zip'\n",
        "files.download(zip_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMahXAoJZBHw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}